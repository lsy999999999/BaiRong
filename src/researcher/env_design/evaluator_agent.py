"""
Evaluator Agent for assessing and selecting research questions and simulation scenarios.

This module provides the EvaluatorAgent class, which is responsible for
evaluating research questions and simulation scenarios generated by the
InspirationAgent and selecting the most promising one for further development.
"""

import json
from typing import Dict, Any, List

from .agent_base import AgentBase


class EvaluatorAgent(AgentBase):
    """
    Agent responsible for evaluating and selecting research questions and scenarios.
    
    The evaluator agent assesses multiple research questions and simulation scenarios
    based on criteria such as feasibility, complexity, relevance, and potential insights,
    and selects the most promising one for further development.
    """
    
    def __init__(self, model_config=None):
        """
        Initialize the evaluator agent.
        
        Args:
            model_config (str, optional): The model configuration name to use.
                If None, the default model will be used.
        """
        super().__init__("Evaluator Agent", model_config)
    
    def _construct_system_prompt(self) -> str:
        """
        Construct the system prompt for the evaluator agent.
        
        Returns:
            str: The system prompt.
        """
        return """You are an expert evaluator of social science research simulations.
Your task is to assess several proposed research questions and simulation scenarios,
and select the most promising one for implementation with language model-based agents.

Evaluate each scenario based on the following criteria:
1. Simulation Feasibility: How feasible is the scenario to implement with language model-based agents?
2. Complexity Management: Is the scenario's complexity appropriate (not too simple, not too complex)?
3. Research Value: Would the simulation provide valuable insights into the broader topic?
4. Agent Design: Are the proposed agent types and interactions well-conceived?
5. Parameter Space: Are the proposed parameters meaningful and manipulable?
6. Expected Insights: Are the expected insights significant and achievable?

For each scenario, provide:
1. A score for each criterion (1-10)
2. Brief justification for each score
3. Overall assessment and recommendations

Then select the most promising scenario, explaining your rationale for the selection.

Format your response as JSON with the following structure:
{
  "evaluations": [
    {
      "question_id": 1,
      "scores": {
        "simulation_feasibility": 8,
        "complexity_management": 7,
        "research_value": 9,
        "agent_design": 8,
        "parameter_space": 7,
        "expected_insights": 8
      },
      "justifications": {
        "simulation_feasibility": "Justification for score",
        ... other justifications ...
      },
      "overall_assessment": "Overall assessment of this scenario",
      "total_score": 47
    },
    ... other evaluations ...
  ],
  "selected_scenario": {
    "question_id": 2,
    "rationale": "Detailed explanation of why this scenario was selected"
  }
}

Be critical but constructive. Focus on selecting a scenario that balances research value with practical feasibility for language model-based agent simulations."""
    
    def _construct_user_prompt(self, research_questions: List[Dict[str, Any]], original_topic: str) -> str:
        """
        Construct the user prompt for the evaluator agent.
        
        Args:
            research_questions (List[Dict[str, Any]]): The list of research questions
                and scenarios to evaluate.
            original_topic (str): The original research topic.
            
        Returns:
            str: The user prompt.
        """
        # Convert research questions to a formatted string
        questions_str = json.dumps(research_questions, indent=2)
        
        return f"""Please evaluate the following research questions and simulation scenarios
that were generated based on this original research topic:

ORIGINAL TOPIC: {original_topic}

RESEARCH QUESTIONS AND SCENARIOS:
{questions_str}

For each scenario:
1. Evaluate it based on the criteria in your instructions
2. Provide justification for your scores
3. Give an overall assessment

Then select the single most promising scenario that would be best to implement
with language model-based agents, providing a detailed rationale for your selection.

Remember to format your response as JSON with the structure specified in your instructions."""
    
    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process the input data and evaluate research questions and scenarios.
        
        Args:
            input_data (Dict[str, Any]): Input data containing research questions
                and the original research topic.
            
        Returns:
            Dict[str, Any]: Output data containing the evaluations and selected scenario.
        """
        # Extract research questions and original topic
        research_questions = input_data.get("research_questions", [])
        original_topic = input_data.get("original_topic", "")
        
        if not research_questions:
            raise ValueError("Research questions are required.")
        
        # Generate response
        system_prompt = self._construct_system_prompt()
        user_prompt = self._construct_user_prompt(research_questions, original_topic)
        
        response = self.generate_response(system_prompt, user_prompt)
        
        # Parse JSON response
        try:
            parsed_response = json.loads(response)
            
            # Get the selected scenario
            selected_id = parsed_response.get("selected_scenario", {}).get("question_id")
            selected_scenario = None
            
            if selected_id:
                # Find the selected scenario in the original research questions
                for question in research_questions:
                    if question.get("id") == selected_id:
                        selected_scenario = question
                        break
            
            return {
                "evaluations": parsed_response.get("evaluations", []),
                "selected_scenario": parsed_response.get("selected_scenario", {}),
                "selected_scenario_details": selected_scenario,
                "original_topic": original_topic
            }
        except json.JSONDecodeError:
            # If JSON parsing fails, try to extract JSON content from text response
            try:
                # Look for JSON content between triple backticks
                if "```json" in response:
                    json_content = response.split("```json")[1].split("```")[0].strip()
                elif "```" in response:
                    json_content = response.split("```")[1].strip()
                else:
                    json_content = response
                
                parsed_response = json.loads(json_content)
                
                # Get the selected scenario
                selected_id = parsed_response.get("selected_scenario", {}).get("question_id")
                selected_scenario = None
                
                if selected_id:
                    # Find the selected scenario in the original research questions
                    for question in research_questions:
                        if question.get("id") == selected_id:
                            selected_scenario = question
                            break
                
                return {
                    "evaluations": parsed_response.get("evaluations", []),
                    "selected_scenario": parsed_response.get("selected_scenario", {}),
                    "selected_scenario_details": selected_scenario,
                    "original_topic": original_topic
                }
            except (json.JSONDecodeError, IndexError):
                # If still fails, return an error message
                raise ValueError(f"Failed to parse agent response as JSON. Response: {response[:200]}...") 