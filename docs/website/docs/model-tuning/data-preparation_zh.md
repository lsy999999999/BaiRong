---
sidebar_position: 2
title: Data Preparation
---

# 数据准备 (Data Preparation)

高质量的数据是成功微调大语言模型（LLM）的基石。本模块简化了数据准备流程，尤其是针对从仿真环境中产生的数据。

## 数据收集流程

系统被设计为可以从仿真环境中自动收集决策数据。当智能体（Agent）在环境中进行交互时，其决策过程中的信息会被捕获并保存。

- **存储路径**：所有收集到的数据会以 JSON 文件的形式统一存储在以下路径：
```

src/envs/\<env\_name\>/datasets/decisions\_\<timestamp\>.json

```
其中 `<env_name>` 是您当前使用的环境名称，`<timestamp>` 是文件创建时的时间戳。

## 数据格式

收集到的数据为 JSON 格式，每个文件包含一个决策记录列表。每个记录都是一个字典，包含了微调所需的关键字段。

**核心字段说明**：

- `prompt`: 提供给模型的输入或指令。这是模型需要理解和响应的上下文。
- `output`: 模型在给定 `prompt` 后生成的实际输出。在 SFT 模式下，这通常是理想的、高质量的“专家”答案。
- `reward`: （可选）一个数值，用于评估 `output` 的好坏。这个字段在 PPO 模式下至关重要，它作为奖励信号来指导模型的优化方向。

**数据示例**：

```json
[
  {
    "prompt": "根据当前市场情况，此时应该购买股票A还是股票B？",
    "output": "考虑到股票A最近的财报表现和市场分析，它可能是一个更稳健的选择。",
    "reward": 0.85
  },
  {
    "prompt": "综合之前的笔试成绩，在求职者A、求职者B和求职者C之中，应该选择谁进入后续的面试环节？",
    "output": "考虑到三位求职者的简历匹配程度与笔试成绩，选择求职者A进入后续面试环节。",
    "reward": 0.95
  }
]
```

在启动微调任务时，您需要通过 `--dataset_path` 参数指定使用哪个（或哪些合并后的）JSON 数据集文件。


