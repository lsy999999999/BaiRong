---
sidebar_position: 5
title: Best Practices
---

# 最佳实践 (Best Practices)

遵循以下最佳实践，可以帮助您更高效、更成功地完成 LLM 微调任务。

## 1. 选择合适的微调模式

- **优先使用 SFT (Supervised Fine-Tuning)**：
  - **场景**：当您拥有高质量的、类似“教科书”的示例数据时（例如，标准的问答对、指令与理想的输出）。
  - **目的**：让模型学习特定的知识、格式或风格。SFT 是构建一个可靠基线模型的 foundational step。
  - **建议**：在尝试 PPO 之前，先通过 SFT 训练一个不错的模型。一个好的 SFT 基线模型是 PPO 成功优化的前提。

- **适当使用 PPO (Proximal Policy Optimization)**：
  - **场景**：当“好”的定义难以用静态数据集描述，但可以通过人类专家评分或模型来量化时。例如，提升输出的安全性、趣味性或遵循某些复杂规则。
  - **目的**：在 SFT 的基础上，根据奖励信号进一步“打磨”模型，使其输出更符合特定偏好。
  - **建议**：PPO 的训练动态更复杂，对超参数和奖励设计更敏感。确保您的奖励信号是稳定且有意义的。

## 2. 有效管理实验

- **为实验命名**：始终使用 `--experiment_name` 参数为您的每一次尝试赋予一个有意义的名称（例如 `sft-base-model-v1` 或 `ppo-reward-scaling-test`）。这能让您在 MLflow 中快速定位和分组实验。
- **从简到繁**：在进行大规模、长时间的训练之前，先使用一小部分数据（例如 10%）进行一次快速的测试，确保整个流程（数据加载、训练、保存）没有 bug。
- **记录关键信息**：虽然 MLflow 会自动记录参数，但您可以在代码或笔记中记录每次实验的“假设”和“结论”，例如，“增加学习率导致模型提前过拟合”。

## 3. 注重数据质量

- **垃圾进，垃圾出 (Garbage In, Garbage Out)**：模型的性能上限很大程度上由数据质量决定。
- **SFT 数据**：确保您的指令清晰、多样，且响应是准确、高质量的。避免数据中的噪声和事实性错误。
- **PPO 数据**：`prompt` 应具有代表性，能够激发模型产生多样化的输出，以便奖励模型能够有效地区分好坏。

## 4. 资源与效率

- **合理分配 GPU**：使用 `--devices` 参数来指定您希望使用的 GPU。如果拥有多张卡，可以并行运行多个实验以提高效率。
- **监控资源使用**：在训练期间，使用 `nvitop` 等工具监控 GPU 的使用情况，确保资源没有被浪费或耗尽。

