---
sidebar_position: 1
title: Model Tuning Overview
---

# 概述 (Overview)

欢迎使用 LLM-Tuning 子模块！本模块旨在为针对模拟过程中的大语言模型（LLM）的微调提供一套完整、易用的解决方案。无论您是想让模型学习新的模拟知识，还是优化其特定模拟场景的性能，本子模块都能提供强有力的支持。

## 核心功能

- **多种微调模式**：内置支持两种主流的微调技术，以适应不同的应用场景：
    - **SFT (Supervised Fine-Tuning)**：监督微调，适用于使用高质量、标注好的“指令-响应”数据集，让模型学习特定的对话风格、知识或任务格式。
    - **PPO (Proximal Policy Optimization)**：近端策略优化，利用强化学习从人类专家标注者或评估模型中获取奖励信号，进一步优化模型的输出，使其更符合人类偏好或特定目标。

- **灵活的调用方式**：
    - **命令行接口**：通过简单的命令即可启动训练任务，方便快速实验和脚本集成。
    - **Python 函数调用**：可作为库轻松集成到您现有的 Python 项目中，实现更复杂的逻辑控制。

- **自动化的工作流**：
    - **数据自动收集**：与仿真环境无缝集成，自动捕获决策数据。
    - **结果自动记录**：训练产物（如模型适配器）会自动保存，并通过 MLflow 全程跟踪实验指标。
    - **模型自动注册**：训练完成的模型会自动登记到模型库，方便后续统一管理和调用。

- **强大的实验跟踪**：
    - 集成 **MLflow**，提供可视化的实验管理界面，让您轻松比较不同训练运行的指标、参数和结果。

本系列文档将引导您完成从数据准备、模型微调、结果评估到最佳实践的整个流程。

